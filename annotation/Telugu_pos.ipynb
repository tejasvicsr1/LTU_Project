{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adBc5fEr4C2g",
        "outputId": "6dcc9b9c-e5ff-44ba-86e2-beb8d63bf09b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root = '/content/drive/My Drive'\n",
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.7/dist-packages (1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.8.1+cu101)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (54.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF_Eh9SR3wj6",
        "outputId": "0131f5e1-3fee-484e-bc99-5496493e2e42"
      },
      "source": [
        "import stanza\n",
        "stanza.download(\"te\")\n",
        "nlp_te = stanza.Pipeline('te')\n",
        "\n",
        "\n",
        "def get_dependencies(doc, n):\n",
        "    def getdeps(i):\n",
        "        deps = []\n",
        "        for head, rel, dep in doc.sentences[i].dependencies:\n",
        "            deps.append((rel, dep.text))\n",
        "        return deps\n",
        "\n",
        "    return [getdeps(i) for i in range(n)]\n",
        "\n",
        "\n",
        "def get_pos_tags(doc, n):\n",
        "    def getpos(i):\n",
        "        tokens = []\n",
        "        for token in doc.sentences[i].words:\n",
        "            tokens.append(token.upos)\n",
        "        return tokens\n",
        "\n",
        "    return [getpos(i) for i in range(n)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 24.1MB/s]                    \n",
            "2021-04-05 16:41:55 INFO: Downloading default packages for language: te (Telugu)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/te/default.zip: 100%|██████████| 353M/353M [01:03<00:00, 5.57MB/s]\n",
            "2021-04-05 16:43:05 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2021-04-05 16:43:05 INFO: Loading these models for language: te (Telugu):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | mtg      |\n",
            "| pos       | mtg      |\n",
            "| lemma     | identity |\n",
            "| depparse  | mtg      |\n",
            "========================\n",
            "\n",
            "2021-04-05 16:43:05 INFO: Use device: cpu\n",
            "2021-04-05 16:43:05 INFO: Loading: tokenize\n",
            "2021-04-05 16:43:05 INFO: Loading: pos\n",
            "2021-04-05 16:43:05 INFO: Loading: lemma\n",
            "2021-04-05 16:43:05 INFO: Loading: depparse\n",
            "2021-04-05 16:43:06 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iopsZdq3oF3"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import spacy\n",
        "#from headers import *\n",
        "#from telugu_helper import *\n",
        "\n",
        "\n",
        "# teldata will store the corpus.\n",
        "# teltags will store the POS tags for all the words in a sentence for all the senteces in the corpus.\n",
        "# teltags = [{\"word_tags\": {\"word\": token, \"POS_TAG\": token.pos_}}] is the structure of engtags.\n",
        "teldata = []\n",
        "teltags = []\n",
        "def print_func(teltags, teldata, g):\n",
        "    g.write(\"telugu sentences and their POS tags.\\n\\n\\n\")\n",
        "    for i in range(len(teltags)):\n",
        "        if i != 500:\n",
        "            g.write(str(i + 1) + \". \" + str(teldata[i]) + \"\\n\\n\")\n",
        "        g.write(\"TAGS:\\n\\n\")\n",
        "        for j in range(len(teltags[i][\"word_tags\"])):\n",
        "            g.write(\"\\t\" + str(j + 1) + \". \" + str(teltags[i][\"word_tags\"][j][\"word\"]) + \" -\\t\" + str(teltags[i][\"word_tags\"][j][\"POS_TAG\"]) + '\\n')\n",
        "            if j == len(teltags[i][\"word_tags\"]) - 1:\n",
        "                g.write('\\n')\n",
        "\n",
        "# Loading the corpus\n",
        "f = open(\"/content/drive/My Drive/conditional_sentences_telugu.txt\", 'r')\n",
        "for sentences in f:\n",
        "    sentences = sentences.rstrip()\n",
        "    teldata.append(sentences)\n",
        "\n",
        "# Finding the POS tags of all the the words in a sentence for all sentences in the corpus.\n",
        "for sentence in teldata:\n",
        "    sentence = nlp_te(sentence)\n",
        "    tagged_sentence = get_pos_tags(sentence, 1)\n",
        "    dep_sentence = get_dependencies(sentence, 1)\n",
        "    sentence_len = len(tagged_sentence[0])\n",
        "    bigtemp = {}\n",
        "    tokens = []\n",
        "    for i in range(sentence_len):\n",
        "        temp = {}\n",
        "        temp[\"word\"] = dep_sentence[0][i][1]\n",
        "        temp[\"POS_TAG\"] = tagged_sentence[0][i]\n",
        "        tokens.append(temp)\n",
        "    bigtemp[\"word_tags\"] = tokens\n",
        "    teltags.append(bigtemp)\n",
        "\n",
        "# Printing the sentences along with the POS tags.\n",
        "g = open(\"/content/drive/My Drive/tel_pos_tags.txt\", 'w')\n",
        "print_func(teltags, teldata, g)"
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}