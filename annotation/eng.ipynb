{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the POS tagger.\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engdata will store the corpus.\n",
    "# engtags will store the POS tags for all the words in a sentence for all the senteces in the corpus.\n",
    "# engtags = [{\"word_tags\": {\"word\": token, \"POS_TAG\": token.pos_}}] is the structure of engtags.\n",
    "engdata = []\n",
    "engtags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the corpus\n",
    "f = open(\"../eng_conditional_sentences.txt\", 'r')\n",
    "for sentences in f:\n",
    "    sentences = sentences.rstrip()\n",
    "    engdata.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the POS tags of all the the words in a sentence for all sentences in the corpus.\n",
    "for sentence in engdata:\n",
    "    engdoc = nlp(sentence)\n",
    "    bigtemp = {}\n",
    "    tokens = []\n",
    "    for token in engdoc:\n",
    "        temp = {}\n",
    "        temp[\"word\"] = token\n",
    "        temp[\"POS_TAG\"] = token.pos_\n",
    "        tokens.append(temp)\n",
    "    bigtemp[\"word_tags\"] = tokens\n",
    "    engtags.append(bigtemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the sentences along with the POS tags.\n",
    "g = open('data/eng_pos_tags.txt', 'w')\n",
    "g.write(\"English sentences and their POS tags.\\n\\n\\n\")\n",
    "for i in range(len(engtags)):\n",
    "    if i != 501:\n",
    "        g.write(str(i + 1) + \". \" + str(engdata[i]) + \"\\n\\n\")\n",
    "    g.write(\"TAGS:\\n\\n\")\n",
    "    for j in range(len(engtags[i][\"word_tags\"])):\n",
    "        g.write(\"\\t\" + str(j + 1) + \". \" + str(engtags[i][\"word_tags\"][j][\"word\"]) + \" -\\t\" + str(engtags[i][\"word_tags\"][j][\"POS_TAG\"]) + '\\n')\n",
    "        if j == len(engtags[i][\"word_tags\"]) - 1:\n",
    "            g.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
